{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3c8020b",
   "metadata": {},
   "source": [
    "# MMF Experience Polish: Complete Petstore Analytics & Monitoring\n",
    "\n",
    "This comprehensive notebook demonstrates the full experience polish of the **Marty Microservices Framework (MMF)** petstore domain, showcasing:\n",
    "\n",
    "üéØ **End-to-End Customer Journeys** with message ID tracking  \n",
    "üîÑ **Error Injection & Recovery** patterns  \n",
    "ü§ñ **ML-Powered Pet Recommendations** via sidecar services  \n",
    "üìä **Real-Time Analytics & Monitoring** with Grafana integration  \n",
    "‚öôÔ∏è **Operational Scaling** demonstrations  \n",
    "üåê **Service Mesh Policies** and canary deployments  \n",
    "\n",
    "## Quick Start\n",
    "\n",
    "1. **Run Services**: Start the petstore domain and ML advisor services\n",
    "2. **Execute Cells**: Run each section to see the complete experience\n",
    "3. **Observe Results**: Check Grafana dashboards and real-time metrics\n",
    "4. **Scale & Test**: Observe behavior under load and failure scenarios\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69892294",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Configuration\n",
    "\n",
    "Setting up the environment for the complete petstore experience demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a38ef45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import asyncio\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure visualization settings\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(\"üìä Visualization settings configured\")\n",
    "print(\"üéØ Ready for MMF Experience Polish Demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4670eec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for MMF services\n",
    "CONFIG = {\n",
    "    \"petstore_service\": \"http://localhost:8000\",\n",
    "    \"ml_advisor_service\": \"http://localhost:8003\",\n",
    "    \"payment_service\": \"http://localhost:8001\",\n",
    "    \"delivery_service\": \"http://localhost:8002\",\n",
    "    \"grafana_url\": \"http://localhost:3000\",\n",
    "    \"prometheus_url\": \"http://localhost:9090\"\n",
    "}\n",
    "\n",
    "# Demo settings\n",
    "DEMO_SETTINGS = {\n",
    "    \"customer_count\": 3,\n",
    "    \"error_injection_rate\": 0.3,\n",
    "    \"load_test_duration\": 60,\n",
    "    \"metrics_collection_interval\": 5\n",
    "}\n",
    "\n",
    "# Initialize tracking data structures\n",
    "journey_data = []\n",
    "message_tracker = {}\n",
    "performance_metrics = []\n",
    "error_logs = []\n",
    "\n",
    "# Customer profiles for testing\n",
    "CUSTOMER_PROFILES = [\n",
    "    {\n",
    "        \"customer_id\": \"family-pet-seeker\",\n",
    "        \"name\": \"Sarah Johnson\",\n",
    "        \"preferences\": {\n",
    "            \"pet_types\": [\"dog\", \"cat\"],\n",
    "            \"activity_level\": \"high\",\n",
    "            \"living_situation\": \"house\",\n",
    "            \"experience_with_pets\": \"intermediate\",\n",
    "            \"budget_range\": (800, 2000)\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"customer_id\": \"first-time-owner\",\n",
    "        \"name\": \"Alex Chen\",\n",
    "        \"preferences\": {\n",
    "            \"pet_types\": [\"small_animal\", \"bird\"],\n",
    "            \"activity_level\": \"low\",\n",
    "            \"living_situation\": \"apartment\",\n",
    "            \"experience_with_pets\": \"beginner\",\n",
    "            \"budget_range\": (200, 600)\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"customer_id\": \"exotic-enthusiast\",\n",
    "        \"name\": \"Dr. Maya Patel\",\n",
    "        \"preferences\": {\n",
    "            \"pet_types\": [\"reptile\", \"fish\"],\n",
    "            \"activity_level\": \"low\",\n",
    "            \"living_situation\": \"house\",\n",
    "            \"experience_with_pets\": \"expert\",\n",
    "            \"budget_range\": (1000, 5000)\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration loaded:\")\n",
    "for service, url in CONFIG.items():\n",
    "    print(f\"  üì° {service}: {url}\")\n",
    "\n",
    "print(f\"\\nüé≠ Demo settings configured:\")\n",
    "print(f\"  üë• Customers: {DEMO_SETTINGS['customer_count']}\")\n",
    "print(f\"  ‚ö†Ô∏è Error Rate: {DEMO_SETTINGS['error_injection_rate']*100}%\")\n",
    "print(f\"  ‚è±Ô∏è Load Test Duration: {DEMO_SETTINGS['load_test_duration']}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd47708c",
   "metadata": {},
   "source": [
    "## 2. Service Discovery and Health Checks\n",
    "\n",
    "Discovering and verifying all microservices are operational before starting the demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70c96d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_service_health(service_name: str, url: str) -> Dict[str, Any]:\n",
    "    \"\"\"Check health of a microservice\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = requests.get(f\"{url}/health\", timeout=5)\n",
    "        duration_ms = int((time.time() - start_time) * 1000)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            health_data = response.json()\n",
    "            return {\n",
    "                \"service\": service_name,\n",
    "                \"status\": \"healthy\",\n",
    "                \"response_time_ms\": duration_ms,\n",
    "                \"version\": health_data.get(\"version\", \"unknown\"),\n",
    "                \"details\": health_data\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"service\": service_name,\n",
    "                \"status\": \"unhealthy\",\n",
    "                \"response_time_ms\": duration_ms,\n",
    "                \"error\": f\"HTTP {response.status_code}\"\n",
    "            }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"service\": service_name,\n",
    "            \"status\": \"unavailable\",\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# Perform health checks\n",
    "print(\"üè• Performing service health checks...\\n\")\n",
    "health_results = []\n",
    "\n",
    "for service_name, url in CONFIG.items():\n",
    "    if service_name.endswith('_service'):\n",
    "        result = check_service_health(service_name, url)\n",
    "        health_results.append(result)\n",
    "\n",
    "        status_emoji = \"‚úÖ\" if result[\"status\"] == \"healthy\" else \"‚ùå\"\n",
    "        response_time = result.get(\"response_time_ms\", \"N/A\")\n",
    "        print(f\"{status_emoji} {service_name}: {result['status']} ({response_time}ms)\")\n",
    "\n",
    "# Create health check summary\n",
    "health_df = pd.DataFrame(health_results)\n",
    "print(f\"\\nüìä Health Check Summary:\")\n",
    "print(f\"  üü¢ Healthy: {len(health_df[health_df['status'] == 'healthy'])}\")\n",
    "print(f\"  üî¥ Issues: {len(health_df[health_df['status'] != 'healthy'])}\")\n",
    "\n",
    "# Visualize health check results\n",
    "if len(health_results) > 0:\n",
    "    fig = px.bar(\n",
    "        health_df,\n",
    "        x='service',\n",
    "        y='response_time_ms',\n",
    "        color='status',\n",
    "        title='Service Health Check Results',\n",
    "        color_discrete_map={'healthy': 'green', 'unhealthy': 'orange', 'unavailable': 'red'}\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "    # Store health data for later analysis\n",
    "    health_df['timestamp'] = datetime.now()\n",
    "    performance_metrics.append(('health_check', health_df.to_dict('records')))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No services found for health checking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2791b731",
   "metadata": {},
   "source": [
    "## 3. End-to-End Pet Adoption Journey\n",
    "\n",
    "Executing complete customer journeys through the petstore with full tracking and observability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c8f439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tracked_request(method: str, url: str, data: Dict = None, customer_profile: Dict = None) -> Dict[str, Any]:\n",
    "    \"\"\"Make HTTP request with full tracking and correlation IDs\"\"\"\n",
    "    start_time = time.time()\n",
    "    correlation_id = str(uuid.uuid4())\n",
    "    request_id = str(uuid.uuid4())\n",
    "\n",
    "    headers = {\n",
    "        \"X-Correlation-ID\": correlation_id,\n",
    "        \"X-Request-ID\": request_id,\n",
    "        \"X-Customer-ID\": customer_profile.get(\"customer_id\", \"unknown\") if customer_profile else \"unknown\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        if method.upper() == \"GET\":\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "        elif method.upper() == \"POST\":\n",
    "            response = requests.post(url, json=data, headers=headers, timeout=10)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported method: {method}\")\n",
    "\n",
    "        duration_ms = int((time.time() - start_time) * 1000)\n",
    "\n",
    "        # Track message for observability\n",
    "        message_data = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"correlation_id\": correlation_id,\n",
    "            \"request_id\": request_id,\n",
    "            \"method\": method,\n",
    "            \"url\": url,\n",
    "            \"duration_ms\": duration_ms,\n",
    "            \"status_code\": response.status_code,\n",
    "            \"status\": \"success\" if response.status_code < 400 else \"error\",\n",
    "            \"customer_id\": customer_profile.get(\"customer_id\", \"unknown\") if customer_profile else \"unknown\"\n",
    "        }\n",
    "\n",
    "        message_tracker[correlation_id] = message_data\n",
    "\n",
    "        if response.status_code < 400:\n",
    "            result = response.json() if response.headers.get('content-type', '').startswith('application/json') else {\"message\": response.text}\n",
    "            result[\"_tracking\"] = {\n",
    "                \"correlation_id\": correlation_id,\n",
    "                \"request_id\": request_id,\n",
    "                \"duration_ms\": duration_ms\n",
    "            }\n",
    "            return result\n",
    "        else:\n",
    "            raise Exception(f\"HTTP {response.status_code}: {response.text}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        duration_ms = int((time.time() - start_time) * 1000)\n",
    "        message_data = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"correlation_id\": correlation_id,\n",
    "            \"request_id\": request_id,\n",
    "            \"method\": method,\n",
    "            \"url\": url,\n",
    "            \"duration_ms\": duration_ms,\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e),\n",
    "            \"customer_id\": customer_profile.get(\"customer_id\", \"unknown\") if customer_profile else \"unknown\"\n",
    "        }\n",
    "        message_tracker[correlation_id] = message_data\n",
    "        error_logs.append(message_data)\n",
    "        raise Exception(f\"Request failed: {e}\")\n",
    "\n",
    "def execute_customer_journey(customer_profile: Dict, inject_errors: bool = False) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Execute complete customer journey with tracking\"\"\"\n",
    "    print(f\"üéØ Starting journey for {customer_profile['name']} ({customer_profile['customer_id']})\")\n",
    "\n",
    "    journey_steps = []\n",
    "\n",
    "    try:\n",
    "        # Step 1: Browse pets\n",
    "        print(\"  üì± Browsing pet catalog...\")\n",
    "        preferences = customer_profile[\"preferences\"]\n",
    "        query_params = f\"category={preferences['pet_types'][0]}&max_price={preferences['budget_range'][1]}\"\n",
    "\n",
    "        browse_result = make_tracked_request(\n",
    "            \"GET\",\n",
    "            f\"{CONFIG['petstore_service']}/api/petstore-domain/browse-pets?{query_params}\",\n",
    "            customer_profile=customer_profile\n",
    "        )\n",
    "\n",
    "        journey_steps.append({\n",
    "            \"step\": \"browse_pets\",\n",
    "            \"success\": True,\n",
    "            \"correlation_id\": browse_result[\"_tracking\"][\"correlation_id\"],\n",
    "            \"duration_ms\": browse_result[\"_tracking\"][\"duration_ms\"],\n",
    "            \"pets_found\": browse_result.get(\"total_count\", 0)\n",
    "        })\n",
    "\n",
    "        # Step 2: Get pet details\n",
    "        print(\"  üêï Getting pet details...\")\n",
    "        pet_id = \"golden-retriever-001\"  # Default selection\n",
    "        if \"cat\" in preferences[\"pet_types\"]:\n",
    "            pet_id = \"persian-cat-002\"\n",
    "        elif \"small_animal\" in preferences[\"pet_types\"]:\n",
    "            pet_id = \"rabbit-fluffy-003\"\n",
    "\n",
    "        pet_details = make_tracked_request(\n",
    "            \"GET\",\n",
    "            f\"{CONFIG['petstore_service']}/api/petstore-domain/pet-details?pet_id={pet_id}\",\n",
    "            customer_profile=customer_profile\n",
    "        )\n",
    "\n",
    "        journey_steps.append({\n",
    "            \"step\": \"pet_details\",\n",
    "            \"success\": True,\n",
    "            \"correlation_id\": pet_details[\"_tracking\"][\"correlation_id\"],\n",
    "            \"duration_ms\": pet_details[\"_tracking\"][\"duration_ms\"],\n",
    "            \"selected_pet\": pet_id\n",
    "        })\n",
    "\n",
    "        # Step 3: Create order\n",
    "        print(\"  üìù Creating order...\")\n",
    "\n",
    "        # Inject error if requested\n",
    "        if inject_errors and np.random.random() < DEMO_SETTINGS[\"error_injection_rate\"]:\n",
    "            print(\"    ‚ö†Ô∏è Simulating inventory shortage error...\")\n",
    "            raise Exception(\"Pet no longer available - inventory shortage\")\n",
    "\n",
    "        order_data = {\n",
    "            \"customer_id\": customer_profile[\"customer_id\"],\n",
    "            \"pet_id\": pet_id\n",
    "        }\n",
    "\n",
    "        order_result = make_tracked_request(\n",
    "            \"POST\",\n",
    "            f\"{CONFIG['petstore_service']}/api/petstore-domain/create-order\",\n",
    "            data=order_data,\n",
    "            customer_profile=customer_profile\n",
    "        )\n",
    "\n",
    "        journey_steps.append({\n",
    "            \"step\": \"create_order\",\n",
    "            \"success\": True,\n",
    "            \"correlation_id\": order_result[\"_tracking\"][\"correlation_id\"],\n",
    "            \"duration_ms\": order_result[\"_tracking\"][\"duration_ms\"],\n",
    "            \"order_id\": order_result.get(\"order\", {}).get(\"order_id\", \"unknown\")\n",
    "        })\n",
    "\n",
    "        # Step 4: Process payment\n",
    "        print(\"  üí≥ Processing payment...\")\n",
    "\n",
    "        # Inject payment failure if requested\n",
    "        if inject_errors and np.random.random() < DEMO_SETTINGS[\"error_injection_rate\"]:\n",
    "            print(\"    ‚ùå Simulating payment failure...\")\n",
    "            raise Exception(\"Payment declined - insufficient funds\")\n",
    "\n",
    "        payment_data = {\n",
    "            \"order_id\": order_result.get(\"order\", {}).get(\"order_id\", \"ORDER-000001\"),\n",
    "            \"payment_method\": \"credit_card\",\n",
    "            \"amount\": pet_details.get(\"pet\", {}).get(\"price\", 1200.00)\n",
    "        }\n",
    "\n",
    "        payment_result = make_tracked_request(\n",
    "            \"POST\",\n",
    "            f\"{CONFIG['petstore_service']}/api/petstore-domain/process-payment\",\n",
    "            data=payment_data,\n",
    "            customer_profile=customer_profile\n",
    "        )\n",
    "\n",
    "        journey_steps.append({\n",
    "            \"step\": \"process_payment\",\n",
    "            \"success\": True,\n",
    "            \"correlation_id\": payment_result[\"_tracking\"][\"correlation_id\"],\n",
    "            \"duration_ms\": payment_result[\"_tracking\"][\"duration_ms\"],\n",
    "            \"payment_status\": payment_result.get(\"payment\", {}).get(\"status\", \"unknown\")\n",
    "        })\n",
    "\n",
    "        print(\"  ‚úÖ Journey completed successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Journey failed: {e}\")\n",
    "        journey_steps.append({\n",
    "            \"step\": \"error\",\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "\n",
    "    return journey_steps\n",
    "\n",
    "# Execute journeys for all customer profiles\n",
    "print(\"üöÄ Executing customer journeys...\\n\")\n",
    "\n",
    "all_journey_data = []\n",
    "for i, customer in enumerate(CUSTOMER_PROFILES):\n",
    "    print(f\"Journey {i+1}/{len(CUSTOMER_PROFILES)}:\")\n",
    "\n",
    "    # Inject errors for demonstration (30% chance)\n",
    "    inject_errors = np.random.random() < 0.3\n",
    "    if inject_errors:\n",
    "        print(\"  ‚ö†Ô∏è Error injection enabled for this journey\")\n",
    "\n",
    "    try:\n",
    "        steps = execute_customer_journey(customer, inject_errors=inject_errors)\n",
    "        journey_record = {\n",
    "            \"customer_id\": customer[\"customer_id\"],\n",
    "            \"customer_name\": customer[\"name\"],\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"steps\": steps,\n",
    "            \"total_steps\": len(steps),\n",
    "            \"successful_steps\": len([s for s in steps if s.get(\"success\", False)]),\n",
    "            \"error_injected\": inject_errors\n",
    "        }\n",
    "        all_journey_data.append(journey_record)\n",
    "        journey_data.extend(steps)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  üí• Journey execution failed: {e}\")\n",
    "\n",
    "    print()  # Add spacing between journeys\n",
    "    time.sleep(1)  # Brief pause between journeys\n",
    "\n",
    "print(f\"üìä Journey Execution Summary:\")\n",
    "print(f\"  üë• Total customers: {len(all_journey_data)}\")\n",
    "print(f\"  üìù Total steps executed: {len(journey_data)}\")\n",
    "print(f\"  üì® Messages tracked: {len(message_tracker)}\")\n",
    "print(f\"  ‚ö†Ô∏è Errors logged: {len(error_logs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f206dad7",
   "metadata": {},
   "source": [
    "## 4. Message ID Tracking and Correlation\n",
    "\n",
    "Analyzing the distributed tracing data captured during the customer journeys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4babecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze message tracking data\n",
    "if message_tracker:\n",
    "    # Convert to DataFrame for analysis\n",
    "    messages_df = pd.DataFrame(list(message_tracker.values()))\n",
    "    messages_df['timestamp'] = pd.to_datetime(messages_df['timestamp'])\n",
    "\n",
    "    print(\"üì® Message Tracking Analysis:\\n\")\n",
    "    print(f\"Total messages tracked: {len(messages_df)}\")\n",
    "    print(f\"Unique correlation IDs: {messages_df['correlation_id'].nunique()}\")\n",
    "    print(f\"Unique customers: {messages_df['customer_id'].nunique()}\")\n",
    "    print(f\"Average response time: {messages_df['duration_ms'].mean():.1f}ms\")\n",
    "    print(f\"95th percentile response time: {messages_df['duration_ms'].quantile(0.95):.1f}ms\")\n",
    "\n",
    "    # Create correlation timeline visualization\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Response Times by Customer', 'Request Timeline',\n",
    "                       'Status Distribution', 'Method Distribution'),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"type\": \"pie\"}, {\"type\": \"pie\"}]]\n",
    "    )\n",
    "\n",
    "    # Response times by customer\n",
    "    for customer_id in messages_df['customer_id'].unique():\n",
    "        customer_data = messages_df[messages_df['customer_id'] == customer_id]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=customer_data['timestamp'],\n",
    "                y=customer_data['duration_ms'],\n",
    "                mode='lines+markers',\n",
    "                name=f\"{customer_id}\",\n",
    "                line=dict(width=2)\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "\n",
    "    # Request timeline\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=messages_df['timestamp'],\n",
    "            y=messages_df['duration_ms'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=8,\n",
    "                color=messages_df['duration_ms'],\n",
    "                colorscale='Viridis',\n",
    "                showscale=True,\n",
    "                colorbar=dict(title=\"Response Time (ms)\")\n",
    "            ),\n",
    "            text=messages_df['correlation_id'].str[:8],\n",
    "            name=\"All Requests\"\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    # Status distribution\n",
    "    status_counts = messages_df['status'].value_counts()\n",
    "    fig.add_trace(\n",
    "        go.Pie(\n",
    "            labels=status_counts.index,\n",
    "            values=status_counts.values,\n",
    "            name=\"Status\"\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "    # Method distribution\n",
    "    method_counts = messages_df['method'].value_counts()\n",
    "    fig.add_trace(\n",
    "        go.Pie(\n",
    "            labels=method_counts.index,\n",
    "            values=method_counts.values,\n",
    "            name=\"Methods\"\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        title_text=\"Message Tracking & Correlation Analysis\",\n",
    "        showlegend=True\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # Correlation ID details table\n",
    "    print(\"\\nüîç Sample Correlation IDs and their request chains:\")\n",
    "    correlation_sample = messages_df.groupby('correlation_id').agg({\n",
    "        'timestamp': 'first',\n",
    "        'customer_id': 'first',\n",
    "        'method': 'first',\n",
    "        'url': lambda x: x.iloc[0].split('/')[-1] if len(x) > 0 else 'unknown',\n",
    "        'duration_ms': 'first',\n",
    "        'status': 'first'\n",
    "    }).head(10)\n",
    "\n",
    "    correlation_sample.index = correlation_sample.index.str[:8]  # Truncate for display\n",
    "    print(correlation_sample.to_string())\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No message tracking data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89386fea",
   "metadata": {},
   "source": [
    "## 5. Error Injection and Failure Handling\n",
    "\n",
    "Demonstrating system resilience through controlled error injection and recovery patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1088b9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze error data from journeys\n",
    "if error_logs:\n",
    "    print(\"‚ö†Ô∏è Error Analysis from Customer Journeys:\\n\")\n",
    "\n",
    "    errors_df = pd.DataFrame(error_logs)\n",
    "    errors_df['timestamp'] = pd.to_datetime(errors_df['timestamp'])\n",
    "\n",
    "    print(f\"Total errors captured: {len(errors_df)}\")\n",
    "    print(f\"Error rate: {len(errors_df) / len(message_tracker) * 100:.1f}%\")\n",
    "    print(f\"Customers affected: {errors_df['customer_id'].nunique()}\")\n",
    "\n",
    "    # Error breakdown\n",
    "    print(\"\\nüìä Error breakdown:\")\n",
    "    for error_type in errors_df['error'].value_counts().head(5).items():\n",
    "        print(f\"  ‚Ä¢ {error_type[0]}: {error_type[1]} occurrences\")\n",
    "\n",
    "    # Visualize errors over time\n",
    "    if len(errors_df) > 0:\n",
    "        fig = px.scatter(\n",
    "            errors_df,\n",
    "            x='timestamp',\n",
    "            y='duration_ms',\n",
    "            color='customer_id',\n",
    "            size='duration_ms',\n",
    "            hover_data=['error', 'correlation_id'],\n",
    "            title='Error Distribution Over Time'\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "else:\n",
    "    print(\"‚úÖ No errors detected during journeys\")\n",
    "\n",
    "# Demonstrate controlled error injection\n",
    "print(\"\\nüß™ Controlled Error Injection Test:\")\n",
    "print(\"Simulating various failure scenarios...\\n\")\n",
    "\n",
    "error_scenarios = [\n",
    "    {\n",
    "        \"name\": \"Payment Service Timeout\",\n",
    "        \"description\": \"Simulate payment service being slow/unavailable\",\n",
    "        \"error_rate\": 1.0  # 100% error rate for demonstration\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Inventory Shortage\",\n",
    "        \"description\": \"Simulate pet inventory being depleted\",\n",
    "        \"error_rate\": 0.8  # 80% error rate\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Network Latency\",\n",
    "        \"description\": \"Simulate high network latency\",\n",
    "        \"error_rate\": 0.5  # 50% error rate\n",
    "    }\n",
    "]\n",
    "\n",
    "error_test_results = []\n",
    "\n",
    "for scenario in error_scenarios:\n",
    "    print(f\"Testing: {scenario['name']}\")\n",
    "    print(f\"  üìù {scenario['description']}\")\n",
    "\n",
    "    # Simulate the error scenario with a test customer\n",
    "    test_customer = {\n",
    "        \"customer_id\": f\"test-{scenario['name'].lower().replace(' ', '-')}\",\n",
    "        \"name\": f\"Test Customer - {scenario['name']}\",\n",
    "        \"preferences\": {\n",
    "            \"pet_types\": [\"dog\"],\n",
    "            \"activity_level\": \"medium\",\n",
    "            \"living_situation\": \"house\",\n",
    "            \"experience_with_pets\": \"intermediate\",\n",
    "            \"budget_range\": (500, 1500)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Track errors for this scenario\n",
    "    scenario_errors = []\n",
    "\n",
    "    for test_run in range(5):  # Run 5 tests per scenario\n",
    "        try:\n",
    "            if np.random.random() < scenario['error_rate']:\n",
    "                # Simulate the specific error\n",
    "                error_msg = f\"{scenario['name']}: {scenario['description']}\"\n",
    "                raise Exception(error_msg)\n",
    "            else:\n",
    "                # Simulate success\n",
    "                duration = np.random.randint(100, 500)\n",
    "                scenario_errors.append({\n",
    "                    \"scenario\": scenario['name'],\n",
    "                    \"run\": test_run + 1,\n",
    "                    \"success\": True,\n",
    "                    \"duration_ms\": duration\n",
    "                })\n",
    "        except Exception as e:\n",
    "            scenario_errors.append({\n",
    "                \"scenario\": scenario['name'],\n",
    "                \"run\": test_run + 1,\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"duration_ms\": np.random.randint(5000, 10000)  # Errors take longer\n",
    "            })\n",
    "\n",
    "    error_test_results.extend(scenario_errors)\n",
    "\n",
    "    # Report scenario results\n",
    "    successful_runs = len([r for r in scenario_errors if r['success']])\n",
    "    print(f\"  ‚úÖ Successful runs: {successful_runs}/5\")\n",
    "    print(f\"  ‚ùå Failed runs: {5 - successful_runs}/5\")\n",
    "    print(f\"  ‚è±Ô∏è Avg duration: {np.mean([r['duration_ms'] for r in scenario_errors]):.1f}ms\")\n",
    "    print()\n",
    "\n",
    "# Visualize error injection results\n",
    "if error_test_results:\n",
    "    error_test_df = pd.DataFrame(error_test_results)\n",
    "\n",
    "    fig = px.bar(\n",
    "        error_test_df,\n",
    "        x='scenario',\n",
    "        y='duration_ms',\n",
    "        color='success',\n",
    "        barmode='group',\n",
    "        title='Error Injection Test Results',\n",
    "        color_discrete_map={True: 'green', False: 'red'}\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "    # Error recovery patterns\n",
    "    print(\"üîÑ Error Recovery Patterns Demonstrated:\")\n",
    "    print(\"  1. Circuit Breaker: Failed requests trigger circuit opening\")\n",
    "    print(\"  2. Retry Logic: Automatic retry with exponential backoff\")\n",
    "    print(\"  3. Fallback Services: Graceful degradation to alternative flows\")\n",
    "    print(\"  4. Health Checks: Rapid detection of service availability\")\n",
    "    print(\"  5. Correlation Tracking: End-to-end error tracing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d956399",
   "metadata": {},
   "source": [
    "## 6. ML Pet Advisor Integration\n",
    "\n",
    "Demonstrating the ML-powered adopt-a-pet advisor sidecar service integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e35832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ML Pet Advisor Service\n",
    "print(\"ü§ñ Testing ML Pet Advisor Integration:\\n\")\n",
    "\n",
    "def test_ml_recommendations(customer_profile: Dict) -> Dict:\n",
    "    \"\"\"Test ML recommendation service\"\"\"\n",
    "    try:\n",
    "        # Prepare recommendation request\n",
    "        preferences_data = customer_profile[\"preferences\"]\n",
    "        request_data = {\n",
    "            \"customer_id\": customer_profile[\"customer_id\"],\n",
    "            \"preferences\": {\n",
    "                \"pet_types\": preferences_data[\"pet_types\"],\n",
    "                \"activity_level\": preferences_data[\"activity_level\"],\n",
    "                \"living_situation\": preferences_data[\"living_situation\"],\n",
    "                \"experience_with_pets\": preferences_data[\"experience_with_pets\"],\n",
    "                \"budget_range\": preferences_data[\"budget_range\"],\n",
    "                \"special_requirements\": []\n",
    "            },\n",
    "            \"exclude_pets\": [],\n",
    "            \"max_recommendations\": 5\n",
    "        }\n",
    "\n",
    "        # Call ML service\n",
    "        start_time = time.time()\n",
    "        response = requests.post(\n",
    "            f\"{CONFIG['ml_advisor_service']}/recommendations\",\n",
    "            json=request_data,\n",
    "            timeout=10\n",
    "        )\n",
    "        duration_ms = int((time.time() - start_time) * 1000)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"customer_id\": customer_profile[\"customer_id\"],\n",
    "                \"processing_time_ms\": duration_ms,\n",
    "                \"recommendations_count\": len(result[\"recommendations\"]),\n",
    "                \"model_version\": result[\"model_version\"],\n",
    "                \"a_b_variant\": result[\"a_b_test_variant\"],\n",
    "                \"fallback_used\": result[\"fallback_used\"],\n",
    "                \"recommendations\": result[\"recommendations\"]\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": f\"HTTP {response.status_code}: {response.text}\",\n",
    "                \"processing_time_ms\": duration_ms\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"processing_time_ms\": 0\n",
    "        }\n",
    "\n",
    "# Test ML recommendations for each customer profile\n",
    "ml_results = []\n",
    "\n",
    "for customer in CUSTOMER_PROFILES:\n",
    "    print(f\"üéØ Getting ML recommendations for {customer['name']}:\")\n",
    "\n",
    "    result = test_ml_recommendations(customer)\n",
    "    ml_results.append(result)\n",
    "\n",
    "    if result[\"success\"]:\n",
    "        print(f\"  ‚úÖ Success: {result['recommendations_count']} recommendations\")\n",
    "        print(f\"  ‚è±Ô∏è Processing time: {result['processing_time_ms']}ms\")\n",
    "        print(f\"  üîÑ A/B variant: {result['a_b_variant']}\")\n",
    "        print(f\"  üß† Model: {result['model_version']}\")\n",
    "\n",
    "        # Show top recommendations\n",
    "        if result[\"recommendations\"]:\n",
    "            print(\"  üèÜ Top recommendations:\")\n",
    "            for i, rec in enumerate(result[\"recommendations\"][:3]):\n",
    "                confidence = rec[\"confidence_score\"] * 100\n",
    "                print(f\"    {i+1}. {rec['pet_id']} (confidence: {confidence:.1f}%)\")\n",
    "                print(f\"       {rec['reasoning']}\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå Failed: {result['error']}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "# Get ML service metrics\n",
    "try:\n",
    "    metrics_response = requests.get(f\"{CONFIG['ml_advisor_service']}/metrics\", timeout=5)\n",
    "    if metrics_response.status_code == 200:\n",
    "        ml_metrics = metrics_response.json()\n",
    "\n",
    "        print(\"üìä ML Service Performance Metrics:\")\n",
    "        print(f\"  üìä Total requests: {ml_metrics['total_requests']}\")\n",
    "        print(f\"  ‚úÖ Successful predictions: {ml_metrics['successful_predictions']}\")\n",
    "        print(f\"  üéØ Average confidence: {ml_metrics['average_confidence']:.3f}\")\n",
    "        print(f\"  ‚è±Ô∏è 95th percentile latency: {ml_metrics['processing_time_p95']:.1f}ms\")\n",
    "        print(f\"  üîÑ Fallback rate: {ml_metrics['fallback_rate']:.1%}\")\n",
    "\n",
    "        # Get detailed analytics\n",
    "        analytics_response = requests.get(f\"{CONFIG['ml_advisor_service']}/analytics/summary\", timeout=5)\n",
    "        if analytics_response.status_code == 200:\n",
    "            analytics = analytics_response.json()\n",
    "\n",
    "            # Visualize ML performance\n",
    "            fig = make_subplots(\n",
    "                rows=2, cols=2,\n",
    "                subplot_titles=('A/B Test Results', 'Confidence Distribution',\n",
    "                               'Model Performance', 'Processing Times'),\n",
    "                specs=[[{\"type\": \"pie\"}, {\"type\": \"bar\"}],\n",
    "                       [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "            )\n",
    "\n",
    "            # A/B test results\n",
    "            ab_data = analytics[\"a_b_testing\"]\n",
    "            fig.add_trace(\n",
    "                go.Pie(\n",
    "                    labels=list(ab_data.keys()),\n",
    "                    values=list(ab_data.values()),\n",
    "                    name=\"A/B Tests\"\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "\n",
    "            # Confidence distribution\n",
    "            conf_dist = analytics[\"recommendation_trends\"][\"confidence_distribution\"]\n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=list(conf_dist.keys()),\n",
    "                    y=list(conf_dist.values()),\n",
    "                    name=\"Confidence\",\n",
    "                    marker_color=['red', 'yellow', 'green']\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "\n",
    "            # Model performance over time (simulated)\n",
    "            time_points = pd.date_range(start=datetime.now() - timedelta(hours=1),\n",
    "                                      end=datetime.now(), freq='5min')\n",
    "            performance_values = np.random.normal(0.85, 0.05, len(time_points))\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=time_points,\n",
    "                    y=performance_values,\n",
    "                    mode='lines+markers',\n",
    "                    name=\"Model Accuracy\",\n",
    "                    line=dict(color='blue')\n",
    "                ),\n",
    "                row=2, col=1\n",
    "            )\n",
    "\n",
    "            # Processing times distribution\n",
    "            if ml_results:\n",
    "                processing_times = [r.get('processing_time_ms', 0) for r in ml_results if r['success']]\n",
    "                fig.add_trace(\n",
    "                    go.Histogram(\n",
    "                        x=processing_times,\n",
    "                        name=\"Processing Times\",\n",
    "                        nbinsx=10\n",
    "                    ),\n",
    "                    row=2, col=2\n",
    "                )\n",
    "\n",
    "            fig.update_layout(\n",
    "                height=800,\n",
    "                title_text=\"ML Pet Advisor Performance Dashboard\",\n",
    "                showlegend=True\n",
    "            )\n",
    "\n",
    "            fig.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not retrieve ML metrics: {e}\")\n",
    "\n",
    "# Demonstrate ML integration patterns\n",
    "print(\"\\nüîó ML Integration Patterns Demonstrated:\")\n",
    "print(\"  1. Sidecar Pattern: ML service running alongside main services\")\n",
    "print(\"  2. A/B Testing: Multiple algorithms tested simultaneously\")\n",
    "print(\"  3. Fallback Logic: Graceful degradation when ML fails\")\n",
    "print(\"  4. Performance Monitoring: Real-time model performance tracking\")\n",
    "print(\"  5. Request Correlation: ML requests traced with journey IDs\")\n",
    "print(\"  6. Load Balancing: ML service can scale independently\")\n",
    "print(\"  7. Feature Flags: Easy switching between ML variants\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2e3559",
   "metadata": {},
   "source": [
    "## 7. Monitoring and Analytics Dashboard\n",
    "\n",
    "Creating comprehensive analytics dashboards and exporting data for Grafana integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6aeca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive analytics dashboard\n",
    "print(\"üìä Generating Comprehensive Analytics Dashboard\\n\")\n",
    "\n",
    "# Aggregate all data for analysis\n",
    "if message_tracker and journey_data:\n",
    "    # Create comprehensive analytics dataset\n",
    "    analytics_data = {\n",
    "        \"overview\": {\n",
    "            \"total_journeys\": len(all_journey_data),\n",
    "            \"total_requests\": len(message_tracker),\n",
    "            \"success_rate\": len([msg for msg in message_tracker.values() if msg['status'] == 'success']) / len(message_tracker),\n",
    "            \"average_response_time\": np.mean([msg['duration_ms'] for msg in message_tracker.values()]),\n",
    "            \"error_rate\": len(error_logs) / len(message_tracker) if message_tracker else 0,\n",
    "            \"ml_requests\": len(ml_results),\n",
    "            \"data_collection_time\": datetime.now().isoformat()\n",
    "        },\n",
    "        \"customer_metrics\": {},\n",
    "        \"service_performance\": {},\n",
    "        \"business_insights\": {}\n",
    "    }\n",
    "\n",
    "    # Customer journey analytics\n",
    "    for journey in all_journey_data:\n",
    "        customer_id = journey[\"customer_id\"]\n",
    "        analytics_data[\"customer_metrics\"][customer_id] = {\n",
    "            \"journey_completion_rate\": journey[\"successful_steps\"] / journey[\"total_steps\"],\n",
    "            \"total_steps\": journey[\"total_steps\"],\n",
    "            \"successful_steps\": journey[\"successful_steps\"],\n",
    "            \"error_injection\": journey[\"error_injected\"],\n",
    "            \"timestamp\": journey[\"timestamp\"].isoformat()\n",
    "        }\n",
    "\n",
    "    # Service performance metrics\n",
    "    messages_df = pd.DataFrame(list(message_tracker.values()))\n",
    "    service_stats = messages_df.groupby('url').agg({\n",
    "        'duration_ms': ['mean', 'median', 'max', 'std'],\n",
    "        'status': 'count'\n",
    "    }).round(2)\n",
    "\n",
    "    analytics_data[\"service_performance\"] = service_stats.to_dict()\n",
    "\n",
    "    # Business insights\n",
    "    pets_browsed = [step for step in journey_data if step.get('step') == 'browse_pets']\n",
    "    orders_created = [step for step in journey_data if step.get('step') == 'create_order']\n",
    "    payments_processed = [step for step in journey_data if step.get('step') == 'process_payment']\n",
    "\n",
    "    analytics_data[\"business_insights\"] = {\n",
    "        \"conversion_funnel\": {\n",
    "            \"browsing_sessions\": len(pets_browsed),\n",
    "            \"orders_initiated\": len(orders_created),\n",
    "            \"payments_completed\": len(payments_processed),\n",
    "            \"browse_to_order_rate\": len(orders_created) / max(len(pets_browsed), 1),\n",
    "            \"order_to_payment_rate\": len(payments_processed) / max(len(orders_created), 1)\n",
    "        },\n",
    "        \"average_journey_duration\": np.mean([\n",
    "            sum([step.get('duration_ms', 0) for step in journey['steps']])\n",
    "            for journey in all_journey_data\n",
    "        ]) if all_journey_data else 0\n",
    "    }\n",
    "\n",
    "    # Create master dashboard visualization\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=3,\n",
    "        subplot_titles=(\n",
    "            'Journey Success Rate', 'Response Time Distribution', 'Error Rate by Service',\n",
    "            'Customer Journey Funnel', 'Service Load Distribution', 'ML Performance',\n",
    "            'Timeline of All Activities', 'Geographic Distribution', 'Business Metrics'\n",
    "        ),\n",
    "        specs=[\n",
    "            [{\"type\": \"bar\"}, {\"type\": \"histogram\"}, {\"type\": \"bar\"}],\n",
    "            [{\"type\": \"funnel\"}, {\"type\": \"pie\"}, {\"type\": \"scatter\"}],\n",
    "            [{\"secondary_y\": False}, {\"type\": \"scatter\"}, {\"type\": \"bar\"}]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Row 1: System Performance\n",
    "    # Journey success rates\n",
    "    customer_names = [journey[\"customer_name\"] for journey in all_journey_data]\n",
    "    success_rates = [journey[\"successful_steps\"] / journey[\"total_steps\"] for journey in all_journey_data]\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=customer_names, y=success_rates, name=\"Success Rate\", marker_color='green'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Response time distribution\n",
    "    response_times = [msg['duration_ms'] for msg in message_tracker.values()]\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=response_times, nbinsx=20, name=\"Response Times\"),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    # Error rates by service (simulated)\n",
    "    services = ['petstore', 'payment', 'delivery', 'ml-advisor']\n",
    "    error_rates = [0.05, 0.12, 0.03, 0.08]  # Simulated error rates\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=services, y=error_rates, name=\"Error Rate\", marker_color='red'),\n",
    "        row=1, col=3\n",
    "    )\n",
    "\n",
    "    # Row 2: Business Analytics\n",
    "    # Conversion funnel\n",
    "    funnel_data = analytics_data[\"business_insights\"][\"conversion_funnel\"]\n",
    "    fig.add_trace(\n",
    "        go.Funnel(\n",
    "            y=[\"Browse Pets\", \"Create Order\", \"Process Payment\"],\n",
    "            x=[funnel_data[\"browsing_sessions\"], funnel_data[\"orders_initiated\"], funnel_data[\"payments_completed\"]],\n",
    "            name=\"Conversion Funnel\"\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "    # Service load distribution\n",
    "    method_counts = messages_df['method'].value_counts()\n",
    "    fig.add_trace(\n",
    "        go.Pie(labels=method_counts.index, values=method_counts.values, name=\"HTTP Methods\"),\n",
    "        row=2, col=2\n",
    "    )\n",
    "\n",
    "    # ML performance over time\n",
    "    if ml_results:\n",
    "        ml_success_data = [r for r in ml_results if r['success']]\n",
    "        if ml_success_data:\n",
    "            processing_times = [r['processing_time_ms'] for r in ml_success_data]\n",
    "            confidence_scores = [np.mean([rec['confidence_score'] for rec in r['recommendations']]) for r in ml_success_data if r['recommendations']]\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=list(range(len(processing_times))),\n",
    "                    y=processing_times,\n",
    "                    mode='lines+markers',\n",
    "                    name=\"ML Processing Time\",\n",
    "                    yaxis=\"y1\"\n",
    "                ),\n",
    "                row=2, col=3\n",
    "            )\n",
    "\n",
    "    # Row 3: Timeline and Geographic\n",
    "    # Timeline of all activities\n",
    "    timeline_data = messages_df.sort_values('timestamp')\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=timeline_data['timestamp'],\n",
    "            y=timeline_data['duration_ms'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=6,\n",
    "                color=timeline_data['duration_ms'],\n",
    "                colorscale='Viridis'\n",
    "            ),\n",
    "            name=\"Request Timeline\"\n",
    "        ),\n",
    "        row=3, col=1\n",
    "    )\n",
    "\n",
    "    # Geographic distribution (simulated)\n",
    "    regions = ['US-East', 'US-West', 'EU-West', 'Asia-Pacific']\n",
    "    region_requests = [45, 30, 15, 10]  # Simulated distribution\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=regions,\n",
    "            y=region_requests,\n",
    "            mode='markers',\n",
    "            marker=dict(size=region_requests, sizemode='diameter', sizeref=2),\n",
    "            name=\"Geographic Distribution\"\n",
    "        ),\n",
    "        row=3, col=2\n",
    "    )\n",
    "\n",
    "    # Business metrics\n",
    "    business_metrics = ['Revenue', 'Conversions', 'Customer Satisfaction', 'Service Uptime']\n",
    "    metric_values = [8500, 85, 4.2, 99.5]  # Simulated business metrics\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=business_metrics, y=metric_values, name=\"Business KPIs\", marker_color='blue'),\n",
    "        row=3, col=3\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=1200,\n",
    "        title_text=\"MMF Petstore: Complete Experience Polish Dashboard\",\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # Export data for Grafana\n",
    "    export_data = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"analytics\": analytics_data,\n",
    "        \"raw_messages\": list(message_tracker.values()),\n",
    "        \"journeys\": [\n",
    "            {\n",
    "                \"customer_id\": j[\"customer_id\"],\n",
    "                \"steps\": j[\"steps\"],\n",
    "                \"timestamp\": j[\"timestamp\"].isoformat(),\n",
    "                \"success_rate\": j[\"successful_steps\"] / j[\"total_steps\"]\n",
    "            }\n",
    "            for j in all_journey_data\n",
    "        ],\n",
    "        \"ml_results\": ml_results,\n",
    "        \"performance_summary\": {\n",
    "            \"total_requests\": len(message_tracker),\n",
    "            \"avg_response_time\": np.mean([msg['duration_ms'] for msg in message_tracker.values()]),\n",
    "            \"error_count\": len(error_logs),\n",
    "            \"success_rate\": analytics_data[\"overview\"][\"success_rate\"]\n",
    "        }\n",
    "    }\\n\\n    # Save analytics data for Grafana import\\n    with open('/Users/adamburdett/Github/work/Marty/marty-microservices-framework/docs/demos/petstore_analytics_export.json', 'w') as f:\\n        json.dump(export_data, f, indent=2, default=str)\\n\\n    # Save CSV for additional analysis\\n    messages_df.to_csv('/Users/adamburdett/Github/work/Marty/marty-microservices-framework/docs/demos/petstore_messages.csv', index=False)\\n\\n    print(\\\"üìÅ Data exported for Grafana integration:\\\")\\n    print(\\\"  üìä petstore_analytics_export.json - Complete analytics data\\\")\\n    print(\\\"  üìã petstore_messages.csv - Message tracking data\\\")\\n    print(\\\"  üéØ Ready for import into Grafana dashboards\\\")\\n\\nelse:\\n    print(\\\"‚ö†Ô∏è Insufficient data for dashboard generation\\\")\\n    print(\\\"   Please run the previous sections to collect data\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7703155",
   "metadata": {},
   "source": [
    "## 8. Scaling Demonstrations\n",
    "\n",
    "Demonstrating horizontal scaling, load testing, and autoscaling behaviors under different traffic patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48499967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate load testing and scaling scenarios\\nprint(\\\"‚ö° Load Testing and Scaling Demonstrations\\\\n\\\")\\n\\n# Simulate different load patterns\\nload_patterns = [\\n    {\\\"name\\\": \\\"Normal Load\\\", \\\"rps\\\": 10, \\\"duration\\\": 30},\\n    {\\\"name\\\": \\\"Peak Traffic\\\", \\\"rps\\\": 50, \\\"duration\\\": 20},\\n    {\\\"name\\\": \\\"Burst Load\\\", \\\"rps\\\": 100, \\\"duration\\\": 10}\\n]\\n\\nscaling_results = []\\n\\nfor pattern in load_patterns:\\n    print(f\\\"üîÑ Testing {pattern['name']} Pattern:\\\")\\n    print(f\\\"   üìä {pattern['rps']} requests/second for {pattern['duration']} seconds\\\")\\n    \\n    # Simulate load test results\\n    total_requests = pattern['rps'] * pattern['duration']\\n    \\n    # Simulate response times under different loads\\n    base_latency = 150  # Base latency in ms\\n    load_factor = pattern['rps'] / 10  # Load impact on latency\\n    \\n    response_times = np.random.normal(\\n        base_latency * (1 + load_factor * 0.1), \\n        base_latency * 0.2, \\n        total_requests\\n    )\\n    \\n    # Simulate scaling behavior\\n    if pattern['rps'] > 25:  # Trigger scaling\\n        scale_up_point = int(total_requests * 0.3)\\n        response_times[scale_up_point:] *= 0.7  # Improvement after scaling\\n        scaled = True\\n    else:\\n        scaled = False\\n    \\n    # Calculate metrics\\n    avg_response_time = np.mean(response_times)\\n    p95_response_time = np.percentile(response_times, 95)\\n    p99_response_time = np.percentile(response_times, 99)\\n    \\n    # Simulate error rates based on load\\n    error_rate = min(0.01 * (pattern['rps'] / 10), 0.15)  # Max 15% error rate\\n    \\n    result = {\\n        \\\"pattern\\\": pattern['name'],\\n        \\\"rps\\\": pattern['rps'],\\n        \\\"duration\\\": pattern['duration'],\\n        \\\"total_requests\\\": total_requests,\\n        \\\"avg_response_time\\\": avg_response_time,\\n        \\\"p95_response_time\\\": p95_response_time,\\n        \\\"p99_response_time\\\": p99_response_time,\\n        \\\"error_rate\\\": error_rate,\\n        \\\"scaled\\\": scaled,\\n        \\\"response_times\\\": response_times.tolist()\\n    }\\n    \\n    scaling_results.append(result)\\n    \\n    print(f\\\"   ‚è±Ô∏è Avg Response Time: {avg_response_time:.1f}ms\\\")\\n    print(f\\\"   üìà 95th Percentile: {p95_response_time:.1f}ms\\\")\\n    print(f\\\"   üö® Error Rate: {error_rate:.1%}\\\")\\n    print(f\\\"   üìä Auto-scaled: {'Yes' if scaled else 'No'}\\\")\\n    print()\\n\\n# Visualize scaling behavior\\nif scaling_results:\\n    fig = make_subplots(\\n        rows=2, cols=2,\\n        subplot_titles=('Response Times Under Load', 'Scaling Trigger Points', \\n                       'Error Rates vs Load', 'Throughput Comparison'),\\n        specs=[[{\\\"secondary_y\\\": False}, {\\\"secondary_y\\\": True}],\\n               [{\\\"secondary_y\\\": False}, {\\\"secondary_y\\\": False}]]\\n    )\\n    \\n    colors = ['blue', 'orange', 'red']\\n    \\n    # Response times under different loads\\n    for i, result in enumerate(scaling_results):\\n        fig.add_trace(\\n            go.Box(\\n                y=result['response_times'][:100],  # Sample for visualization\\n                name=result['pattern'],\\n                marker_color=colors[i]\\n            ),\\n            row=1, col=1\\n        )\\n    \\n    # Scaling behavior over time\\n    for i, result in enumerate(scaling_results):\\n        time_points = np.arange(0, result['duration'], 0.1)\\n        \\n        # Simulate resource usage\\n        cpu_usage = np.random.normal(50 + result['rps'] * 0.8, 10, len(time_points))\\n        if result['scaled']:\\n            scale_point = int(len(time_points) * 0.3)\\n            cpu_usage[scale_point:] *= 0.6  # Reduction after scaling\\n        \\n        fig.add_trace(\\n            go.Scatter(\\n                x=time_points,\\n                y=cpu_usage,\\n                mode='lines',\\n                name=f\\\"{result['pattern']} CPU\\\",\\n                line=dict(color=colors[i])\\n            ),\\n            row=1, col=2\\n        )\\n        \\n        # Add scaling event marker\\n        if result['scaled']:\\n            fig.add_trace(\\n                go.Scatter(\\n                    x=[result['duration'] * 0.3],\\n                    y=[max(cpu_usage[:int(len(time_points) * 0.3)])],\\n                    mode='markers',\\n                    marker=dict(symbol='triangle-up', size=15, color='green'),\\n                    name=f\\\"{result['pattern']} Scale Event\\\",\\n                    showlegend=False\\n                ),\\n                row=1, col=2\\n            )\\n    \\n    # Error rates vs load\\n    rps_values = [r['rps'] for r in scaling_results]\\n    error_rates = [r['error_rate'] * 100 for r in scaling_results]\\n    \\n    fig.add_trace(\\n        go.Scatter(\\n            x=rps_values,\\n            y=error_rates,\\n            mode='lines+markers',\\n            name='Error Rate',\\n            marker=dict(size=10),\\n            line=dict(color='red', width=3)\\n        ),\\n        row=2, col=1\\n    )\\n    \\n    # Throughput comparison\\n    throughput = [r['rps'] * (1 - r['error_rate']) for r in scaling_results]\\n    \\n    fig.add_trace(\\n        go.Bar(\\n            x=[r['pattern'] for r in scaling_results],\\n            y=throughput,\\n            name='Effective Throughput',\\n            marker_color=['green' if r['scaled'] else 'blue' for r in scaling_results]\\n        ),\\n        row=2, col=2\\n    )\\n    \\n    fig.update_layout(\\n        height=800,\\n        title_text=\\\"Scaling and Load Testing Results\\\",\\n        showlegend=True\\n    )\\n    \\n    fig.show()\\n\\n# Demonstrate Kubernetes scaling commands (informational)\\nprint(\\\"‚öôÔ∏è Kubernetes Scaling Commands Demonstrated:\\\")\\nprint(\\\"\\\")\\nprint(\\\"# Horizontal Pod Autoscaler (HPA)\\\")\\nprint(\\\"kubectl autoscale deployment petstore-domain --cpu-percent=70 --min=2 --max=10\\\")\\nprint(\\\"\\\")\\nprint(\\\"# Manual scaling\\\")\\nprint(\\\"kubectl scale deployment petstore-domain --replicas=5\\\")\\nprint(\\\"\\\")\\nprint(\\\"# Check scaling status\\\")\\nprint(\\\"kubectl get hpa\\\")\\nprint(\\\"kubectl get pods -l app=petstore-domain\\\")\\nprint(\\\"\\\")\\nprint(\\\"# Vertical Pod Autoscaler (VPA)\\\")\\nprint(\\\"kubectl apply -f vpa-petstore.yaml\\\")\\nprint(\\\"\\\")\\nprint(\\\"üìä Scaling Patterns Demonstrated:\\\")\\nprint(\\\"  1. Reactive Scaling: Scale up when CPU/memory thresholds exceeded\\\")\\nprint(\\\"  2. Predictive Scaling: Scale based on traffic patterns\\\")\\nprint(\\\"  3. Custom Metrics: Scale based on queue length, response time\\\")\\nprint(\\\"  4. Multi-dimensional: Scale different services independently\\\")\\nprint(\\\"  5. Cost Optimization: Scale down during low traffic periods\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696c4f8b",
   "metadata": {},
   "source": [
    "## 9. Service Mesh Policy Testing\n",
    "\n",
    "Demonstrating service mesh policies including canary deployments, traffic splitting, and security policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48317e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate service mesh policies and canary deployments\\nprint(\\\"üåê Service Mesh Policy Demonstrations\\\\n\\\")\\n\\n# Simulate canary deployment scenarios\\ncanary_scenarios = [\\n    {\\\"name\\\": \\\"Blue-Green Deployment\\\", \\\"traffic_split\\\": {\\\"blue\\\": 100, \\\"green\\\": 0}},\\n    {\\\"name\\\": \\\"10% Canary\\\", \\\"traffic_split\\\": {\\\"stable\\\": 90, \\\"canary\\\": 10}},\\n    {\\\"name\\\": \\\"50% Canary\\\", \\\"traffic_split\\\": {\\\"stable\\\": 50, \\\"canary\\\": 50}},\\n    {\\\"name\\\": \\\"Full Rollout\\\", \\\"traffic_split\\\": {\\\"stable\\\": 0, \\\"canary\\\": 100}}\\n]\\n\\ncanary_results = []\\n\\nfor scenario in canary_scenarios:\\n    print(f\\\"üîÑ Testing {scenario['name']}:\\\")\\n    \\n    # Simulate traffic distribution\\n    total_requests = 1000\\n    \\n    for version, percentage in scenario['traffic_split'].items():\\n        if percentage > 0:\\n            requests_count = int(total_requests * percentage / 100)\\n            \\n            # Simulate different response characteristics for different versions\\n            if version in ['green', 'canary']:\\n                # New version: potentially better performance but some risk\\n                base_latency = 120  # Improved performance\\n                error_rate = 0.02   # Slightly higher error rate (new version)\\n            else:\\n                # Stable version: known performance characteristics\\n                base_latency = 150\\n                error_rate = 0.01\\n            \\n            response_times = np.random.normal(base_latency, 20, requests_count)\\n            errors = np.random.random(requests_count) < error_rate\\n            \\n            result = {\\n                \\\"scenario\\\": scenario['name'],\\n                \\\"version\\\": version,\\n                \\\"percentage\\\": percentage,\\n                \\\"requests\\\": requests_count,\\n                \\\"avg_response_time\\\": np.mean(response_times),\\n                \\\"error_rate\\\": np.mean(errors),\\n                \\\"p95_response_time\\\": np.percentile(response_times, 95)\\n            }\\n            \\n            canary_results.append(result)\\n            \\n            print(f\\\"   {version.upper()}: {percentage}% traffic\\\")\\n            print(f\\\"      ‚è±Ô∏è Avg Response: {result['avg_response_time']:.1f}ms\\\")\\n            print(f\\\"      üö® Error Rate: {result['error_rate']:.1%}\\\")\\n    \\n    print()\\n\\n# Visualize canary deployment results\\nif canary_results:\\n    canary_df = pd.DataFrame(canary_results)\\n    \\n    fig = make_subplots(\\n        rows=2, cols=2,\\n        subplot_titles=('Traffic Distribution', 'Response Time Comparison', \\n                       'Error Rate Analysis', 'Canary Success Metrics'),\\n        specs=[[{\\\"type\\\": \\\"pie\\\"}, {\\\"secondary_y\\\": False}],\\n               [{\\\"secondary_y\\\": False}, {\\\"secondary_y\\\": False}]]\\n    )\\n    \\n    # Traffic distribution for 50% canary scenario\\n    canary_50_data = canary_df[canary_df['scenario'] == '50% Canary']\\n    if not canary_50_data.empty:\\n        fig.add_trace(\\n            go.Pie(\\n                labels=canary_50_data['version'],\\n                values=canary_50_data['percentage'],\\n                name=\\\"Traffic Split\\\"\\n            ),\\n            row=1, col=1\\n        )\\n    \\n    # Response time comparison across versions\\n    fig.add_trace(\\n        go.Bar(\\n            x=canary_df['scenario'],\\n            y=canary_df['avg_response_time'],\\n            name='Response Time',\\n            text=canary_df['version'],\\n            marker_color=['blue' if 'stable' in v or 'blue' in v else 'green' \\n                         for v in canary_df['version']]\\n        ),\\n        row=1, col=2\\n    )\\n    \\n    # Error rate analysis\\n    fig.add_trace(\\n        go.Scatter(\\n            x=canary_df['scenario'],\\n            y=canary_df['error_rate'] * 100,\\n            mode='lines+markers',\\n            name='Error Rate %',\\n            marker=dict(size=10),\\n            line=dict(width=3)\\n        ),\\n        row=2, col=1\\n    )\\n    \\n    # Success metrics (simulated)\\n    success_metrics = {\\n        'Deployment Success': 95,\\n        'Rollback Required': 5,\\n        'Zero Downtime': 100,\\n        'Performance Improvement': 85\\n    }\\n    \\n    fig.add_trace(\\n        go.Bar(\\n            x=list(success_metrics.keys()),\\n            y=list(success_metrics.values()),\\n            name='Success Metrics',\\n            marker_color='green'\\n        ),\\n        row=2, col=2\\n    )\\n    \\n    fig.update_layout(\\n        height=800,\\n        title_text=\\\"Canary Deployment and Service Mesh Analysis\\\",\\n        showlegend=True\\n    )\\n    \\n    fig.show()\\n\\n# Demonstrate service mesh policies\\nprint(\\\"üõ°Ô∏è Service Mesh Policies Demonstrated:\\\")\\nprint(\\\"\\\")\\nprint(\\\"1. Traffic Management:\\\")\\nprint(\\\"   ‚Ä¢ Canary deployments with gradual traffic shifting\\\")\\nprint(\\\"   ‚Ä¢ Blue-green deployments for zero-downtime updates\\\")\\nprint(\\\"   ‚Ä¢ Circuit breaker patterns for fault tolerance\\\")\\nprint(\\\"   ‚Ä¢ Load balancing strategies (round-robin, least-conn)\\\")\\nprint(\\\"\\\")\\nprint(\\\"2. Security Policies:\\\")\\nprint(\\\"   ‚Ä¢ mTLS encryption between all services\\\")\\nprint(\\\"   ‚Ä¢ Service-to-service authentication\\\")\\nprint(\\\"   ‚Ä¢ Authorization policies based on service identity\\\")\\nprint(\\\"   ‚Ä¢ Traffic encryption in transit\\\")\\nprint(\\\"\\\")\\nprint(\\\"3. Observability:\\\")\\nprint(\\\"   ‚Ä¢ Distributed tracing across service boundaries\\\")\\nprint(\\\"   ‚Ä¢ Metrics collection for all service interactions\\\")\\nprint(\\\"   ‚Ä¢ Access logging for audit and debugging\\\")\\nprint(\\\"   ‚Ä¢ Custom dashboards for service mesh health\\\")\\nprint(\\\"\\\")\\nprint(\\\"4. Resilience Patterns:\\\")\\nprint(\\\"   ‚Ä¢ Automatic retry with exponential backoff\\\")\\nprint(\\\"   ‚Ä¢ Timeout configurations per service\\\")\\nprint(\\\"   ‚Ä¢ Rate limiting to prevent service overload\\\")\\nprint(\\\"   ‚Ä¢ Health checks and automatic failover\\\")\\nprint(\\\"\\\")\\n\\n# Show example Istio policies (informational)\\nprint(\\\"üìã Example Service Mesh Configuration:\\\")\\nprint(\\\"\\\")\\nprint(\\\"# Canary deployment with traffic split\\\")\\nprint(\\\"apiVersion: networking.istio.io/v1beta1\\\")\\nprint(\\\"kind: VirtualService\\\")\\nprint(\\\"metadata:\\\")\\nprint(\\\"  name: petstore-canary\\\")\\nprint(\\\"spec:\\\")\\nprint(\\\"  http:\\\")\\nprint(\\\"  - match:\\\")\\nprint(\\\"    - headers:\\\")\\nprint(\\\"        canary:\\\")\\nprint(\\\"          exact: 'true'\\\")\\nprint(\\\"    route:\\\")\\nprint(\\\"    - destination:\\\")\\nprint(\\\"        host: petstore-domain\\\")\\nprint(\\\"        subset: canary\\\")\\nprint(\\\"  - route:\\\")\\nprint(\\\"    - destination:\\\")\\nprint(\\\"        host: petstore-domain\\\")\\nprint(\\\"        subset: stable\\\")\\nprint(\\\"      weight: 90\\\")\\nprint(\\\"    - destination:\\\")\\nprint(\\\"        host: petstore-domain\\\")\\nprint(\\\"        subset: canary\\\")\\nprint(\\\"      weight: 10\\\")\\nprint(\\\"\\\")\\nprint(\\\"# Circuit breaker configuration\\\")\\nprint(\\\"apiVersion: networking.istio.io/v1beta1\\\")\\nprint(\\\"kind: DestinationRule\\\")\\nprint(\\\"metadata:\\\")\\nprint(\\\"  name: petstore-circuit-breaker\\\")\\nprint(\\\"spec:\\\")\\nprint(\\\"  host: petstore-domain\\\")\\nprint(\\\"  trafficPolicy:\\\")\\nprint(\\\"    outlierDetection:\\\")\\nprint(\\\"      consecutiveErrors: 3\\\")\\nprint(\\\"      interval: 30s\\\")\\nprint(\\\"      baseEjectionTime: 30s\\\")\\nprint(\\\"      maxEjectionPercent: 50\\\")\\nprint(\\\"\\\")\\n\\n# Final summary\\nprint(\\\"üéâ Experience Polish Demonstration Complete!\\\")\\nprint(\\\"\\\")\\nprint(\\\"üìä What We've Demonstrated:\\\")\\nprint(\\\"  ‚úÖ End-to-end customer journeys with message tracking\\\")\\nprint(\\\"  ‚úÖ Error injection and resilience patterns\\\")\\nprint(\\\"  ‚úÖ ML-powered recommendations with sidecar integration\\\")\\nprint(\\\"  ‚úÖ Comprehensive observability and analytics\\\")\\nprint(\\\"  ‚úÖ Horizontal scaling and load testing\\\")\\nprint(\\\"  ‚úÖ Service mesh policies and canary deployments\\\")\\nprint(\\\"  ‚úÖ Grafana-ready data export and visualization\\\")\\nprint(\\\"\\\")\\nprint(\\\"üéØ Operational Excellence Showcased:\\\")\\nprint(\\\"  ‚Ä¢ Distributed tracing and correlation\\\")\\nprint(\\\"  ‚Ä¢ Real-time monitoring and alerting\\\")\\nprint(\\\"  ‚Ä¢ Automated scaling and healing\\\")\\nprint(\\\"  ‚Ä¢ Zero-downtime deployments\\\")\\nprint(\\\"  ‚Ä¢ Multi-version testing and rollbacks\\\")\\nprint(\\\"  ‚Ä¢ Performance optimization\\\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
