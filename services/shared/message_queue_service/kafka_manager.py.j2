"""
Kafka Manager for Message Queue Service
"""

import asyncio
import logging
from typing import Optional, Dict, Any, List, Callable
import json
from datetime import datetime, timezone

from kafka import KafkaProducer, KafkaConsumer, KafkaAdminClient
from kafka.admin import ConfigResource, ConfigResourceType, NewTopic
from kafka.errors import KafkaError, TopicAlreadyExistsError

from src.{{service_package}}.app.core.config import {{service_class}}ServiceConfig
from src.{{service_package}}.app.core.models import Message, MessageMetadata

logger = logging.getLogger(__name__)


class KafkaManager:
    """Manages Kafka connections and operations."""

    def __init__(self, config: {{service_class}}ServiceConfig):
        """Initialize the Kafka manager."""
        self.config = config
        self.producer: Optional[KafkaProducer] = None
        self.consumers: Dict[str, KafkaConsumer] = {}
        self.admin_client: Optional[KafkaAdminClient] = None
        self.is_initialized = False

    async def initialize(self) -> None:
        """Initialize Kafka connections."""
        if not self.config.kafka_enabled:
            logger.warning("Kafka is not enabled")
            return

        try:
            logger.info("Initializing Kafka manager...")

            # Initialize admin client
            await self._initialize_admin_client()

            # Create topics if they don't exist
            await self._create_topics()

            # Initialize producer
            await self._initialize_producer()

            self.is_initialized = True
            logger.info("Kafka manager initialized successfully")

        except Exception as e:
            logger.error(f"Failed to initialize Kafka manager: {e}")
            raise

    async def _initialize_admin_client(self) -> None:
        """Initialize Kafka admin client."""
        try:
            kafka_config = self.config.get_kafka_config()
            self.admin_client = KafkaAdminClient(**kafka_config)

            # Test connection
            metadata = self.admin_client.describe_cluster()
            logger.info(f"Connected to Kafka cluster: {metadata}")

        except Exception as e:
            logger.error(f"Failed to initialize Kafka admin client: {e}")
            raise

    async def _create_topics(self) -> None:
        """Create required Kafka topics."""
        if not self.admin_client:
            logger.warning("Admin client not initialized, skipping topic creation")
            return

        try:
            topic_configs = self.config.get_topic_configs()
            new_topics = []

            for topic_config in topic_configs:
                new_topic = NewTopic(
                    name=topic_config["name"],
                    num_partitions=topic_config["partitions"],
                    replication_factor=topic_config["replication_factor"],
                    topic_configs=topic_config.get("config", {})
                )
                new_topics.append(new_topic)

            if new_topics:
                result = self.admin_client.create_topics(new_topics, validate_only=False)

                for topic, future in result.topic_errors.items():
                    try:
                        future.result()  # The result itself is None
                        logger.info(f"Created topic: {topic}")
                    except TopicAlreadyExistsError:
                        logger.debug(f"Topic already exists: {topic}")
                    except Exception as e:
                        logger.error(f"Failed to create topic {topic}: {e}")

        except Exception as e:
            logger.error(f"Failed to create topics: {e}")
            # Don't raise here as topics might already exist

    async def _initialize_producer(self) -> None:
        """Initialize Kafka producer."""
        try:
            producer_config = self.config.get_kafka_producer_config()

            # Add serialization
            producer_config.update({
                'value_serializer': lambda v: json.dumps(v, default=str).encode('utf-8'),
                'key_serializer': lambda k: k.encode('utf-8') if k else None,
            })

            self.producer = KafkaProducer(**producer_config)
            logger.info("Kafka producer initialized")

        except Exception as e:
            logger.error(f"Failed to initialize Kafka producer: {e}")
            raise

    def create_consumer(self, topics: List[str], group_id: Optional[str] = None) -> KafkaConsumer:
        """Create a Kafka consumer for specified topics.

        Args:
            topics: List of topics to subscribe to
            group_id: Consumer group ID (optional)

        Returns:
            KafkaConsumer instance
        """
        try:
            consumer_config = self.config.get_kafka_consumer_config()

            if group_id:
                consumer_config["group_id"] = group_id

            # Add deserialization
            consumer_config.update({
                'value_deserializer': lambda m: json.loads(m.decode('utf-8')) if m else None,
                'key_deserializer': lambda k: k.decode('utf-8') if k else None,
            })

            consumer = KafkaConsumer(*topics, **consumer_config)

            consumer_id = f"{group_id or 'default'}_{len(self.consumers)}"
            self.consumers[consumer_id] = consumer

            logger.info(f"Created Kafka consumer {consumer_id} for topics: {topics}")
            return consumer

        except Exception as e:
            logger.error(f"Failed to create Kafka consumer: {e}")
            raise

    async def produce_message(
        self,
        topic: str,
        message: Message,
        key: Optional[str] = None,
        partition: Optional[int] = None,
        headers: Optional[Dict[str, bytes]] = None
    ) -> bool:
        """Produce a message to Kafka topic.

        Args:
            topic: Topic name
            message: Message to send
            key: Message key for partitioning
            partition: Specific partition to send to
            headers: Message headers

        Returns:
            True if message was sent successfully
        """
        if not self.producer:
            logger.error("Kafka producer not initialized")
            return False

        try:
            # Prepare message payload
            payload = {
                "id": message.id,
                "type": message.type,
                "data": message.data,
                "metadata": {
                    "timestamp": message.metadata.timestamp.isoformat(),
                    "source": message.metadata.source,
                    "version": message.metadata.version,
                    "correlation_id": message.metadata.correlation_id,
                    "causation_id": message.metadata.causation_id,
                    "user_id": message.metadata.user_id,
                    "session_id": message.metadata.session_id,
                    "trace_id": message.metadata.trace_id,
                    "span_id": message.metadata.span_id,
                    "headers": message.metadata.headers,
                    "tags": message.metadata.tags
                }
            }

            # Send message
            future = self.producer.send(
                topic=topic,
                value=payload,
                key=key,
                partition=partition,
                headers=[(k, v) for k, v in (headers or {}).items()]
            )

            # Wait for confirmation (optional - for at-least-once delivery)
            if self.config.default_delivery_guarantee.value != "at_most_once":
                record_metadata = future.get(timeout=30)
                logger.debug(f"Message sent to {record_metadata.topic}:{record_metadata.partition}:{record_metadata.offset}")

            return True

        except Exception as e:
            logger.error(f"Failed to produce message to topic {topic}: {e}")
            return False

    async def consume_messages(
        self,
        consumer: KafkaConsumer,
        message_handler: Callable[[Message], bool],
        batch_size: int = 100,
        timeout_ms: int = 1000
    ) -> None:
        """Consume messages from Kafka topics.

        Args:
            consumer: KafkaConsumer instance
            message_handler: Function to handle each message
            batch_size: Maximum messages to process in one batch
            timeout_ms: Consumer poll timeout
        """
        try:
            logger.info("Starting Kafka message consumption")

            while True:
                # Poll for messages
                message_batch = consumer.poll(timeout_ms=timeout_ms, max_records=batch_size)

                if not message_batch:
                    await asyncio.sleep(0.1)  # Short sleep to prevent busy waiting
                    continue

                # Process messages
                for topic_partition, messages in message_batch.items():
                    for kafka_message in messages:
                        try:
                            # Convert Kafka message to our Message format
                            message = self._kafka_message_to_message(kafka_message)

                            # Handle the message
                            success = await self._handle_message_with_retry(message, message_handler)

                            if not success:
                                logger.error(f"Failed to process message after retries: {message.id}")
                                await self._send_to_dlq(message, topic_partition.topic)

                        except Exception as e:
                            logger.error(f"Error processing message: {e}")
                            continue

                # Commit offsets manually if auto-commit is disabled
                if not self.config.kafka_consumer_enable_auto_commit:
                    try:
                        consumer.commit()
                    except Exception as e:
                        logger.error(f"Failed to commit offsets: {e}")

        except Exception as e:
            logger.error(f"Error in Kafka message consumption: {e}")
            raise

    def _kafka_message_to_message(self, kafka_message) -> Message:
        """Convert Kafka message to our Message format."""
        payload = kafka_message.value

        metadata = MessageMetadata(
            timestamp=datetime.fromisoformat(payload["metadata"]["timestamp"]),
            source=payload["metadata"]["source"],
            version=payload["metadata"]["version"],
            correlation_id=payload["metadata"]["correlation_id"],
            causation_id=payload["metadata"]["causation_id"],
            user_id=payload["metadata"]["user_id"],
            session_id=payload["metadata"]["session_id"],
            trace_id=payload["metadata"]["trace_id"],
            span_id=payload["metadata"]["span_id"],
            headers=payload["metadata"]["headers"],
            tags=payload["metadata"]["tags"]
        )

        return Message(
            id=payload["id"],
            type=payload["type"],
            data=payload["data"],
            metadata=metadata
        )

    async def _handle_message_with_retry(
        self,
        message: Message,
        handler: Callable[[Message], bool],
        max_retries: Optional[int] = None
    ) -> bool:
        """Handle message with retry logic."""
        max_retries = max_retries or self.config.message_retry_attempts
        delay = self.config.message_retry_delay_ms / 1000.0

        for attempt in range(max_retries + 1):
            try:
                success = handler(message)
                if success:
                    return True

                if attempt < max_retries:
                    logger.warning(f"Message processing failed, retrying in {delay}s (attempt {attempt + 1}/{max_retries})")
                    await asyncio.sleep(delay)
                    delay = min(delay * self.config.message_retry_backoff_multiplier,
                              self.config.message_max_retry_delay_ms / 1000.0)

            except Exception as e:
                logger.error(f"Error handling message (attempt {attempt + 1}): {e}")
                if attempt < max_retries:
                    await asyncio.sleep(delay)
                    delay = min(delay * self.config.message_retry_backoff_multiplier,
                              self.config.message_max_retry_delay_ms / 1000.0)

        return False

    async def _send_to_dlq(self, message: Message, original_topic: str) -> None:
        """Send message to dead letter queue."""
        if not self.config.dlq_enabled:
            return

        dlq_topic = f"{original_topic}{self.config.dlq_topic_suffix}"

        # Add DLQ metadata
        message.metadata.headers = message.metadata.headers or {}
        message.metadata.headers.update({
            "original_topic": original_topic,
            "dlq_timestamp": datetime.now(timezone.utc).isoformat(),
            "failure_reason": "max_retries_exceeded"
        })

        await self.produce_message(dlq_topic, message)
        logger.info(f"Message {message.id} sent to DLQ: {dlq_topic}")

    async def get_topic_metadata(self, topic: str) -> Dict[str, Any]:
        """Get metadata for a specific topic."""
        try:
            metadata = self.admin_client.describe_topics([topic])
            return metadata.get(topic, {})
        except Exception as e:
            logger.error(f"Failed to get topic metadata for {topic}: {e}")
            return {}

    async def get_consumer_group_info(self, group_id: str) -> Dict[str, Any]:
        """Get information about a consumer group."""
        try:
            groups = self.admin_client.describe_consumer_groups([group_id])
            return groups.get(group_id, {})
        except Exception as e:
            logger.error(f"Failed to get consumer group info for {group_id}: {e}")
            return {}

    async def shutdown(self) -> None:
        """Shutdown Kafka connections."""
        logger.info("Shutting down Kafka manager...")

        try:
            # Close all consumers
            for consumer_id, consumer in self.consumers.items():
                try:
                    consumer.close()
                    logger.debug(f"Closed Kafka consumer: {consumer_id}")
                except Exception as e:
                    logger.error(f"Error closing consumer {consumer_id}: {e}")

            self.consumers.clear()

            # Close producer
            if self.producer:
                try:
                    self.producer.flush()  # Ensure all messages are sent
                    self.producer.close()
                    logger.debug("Closed Kafka producer")
                except Exception as e:
                    logger.error(f"Error closing producer: {e}")

            # Close admin client
            if self.admin_client:
                try:
                    self.admin_client.close()
                    logger.debug("Closed Kafka admin client")
                except Exception as e:
                    logger.error(f"Error closing admin client: {e}")

            self.is_initialized = False
            logger.info("Kafka manager shutdown complete")

        except Exception as e:
            logger.error(f"Error during Kafka shutdown: {e}")


# Global Kafka manager instance
_kafka_manager: Optional[KafkaManager] = None


def get_kafka_manager() -> KafkaManager:
    """Get the global Kafka manager instance."""
    global _kafka_manager
    if _kafka_manager is None:
        config = {{service_class}}ServiceConfig()
        _kafka_manager = KafkaManager(config)
    return _kafka_manager
